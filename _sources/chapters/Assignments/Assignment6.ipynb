{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI820A_Bp_NH"
      },
      "source": [
        "# Assignment 6\n",
        "\n",
        "\n",
        "\n",
        "Instructions: This problem set should be done in a **group**.\n",
        "\n",
        "Your group was assigned at orientation and you can find it in blackboard as well.\n",
        "\n",
        "You will use this same group to do your final project.\n",
        "\n",
        "Answer each question in the designated space below.\n",
        "\n",
        "After you are done. save and upload in blackboard.\n",
        "\n",
        "Please check that you are submitting the correct file. One way to avoid mistakes is to save it with a different name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVAN3J3Vp_NJ"
      },
      "source": [
        "# Names of your group members\n",
        "\n",
        "Please write names below\n",
        "\n",
        "* [Name]:\n",
        "* [Name]:\n",
        "* [Name]:\n",
        "* [Name]:\n",
        "* [Name]:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srfn2UWb1xTm"
      },
      "source": [
        "# Discussion Forum Assignment\n",
        "\n",
        "\n",
        "Do the Value investing assignment in the discussion forum\n",
        "\n",
        "\n",
        "https://edstem.org/us/courses/30665/discussion/1987699\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekyShyt91xTn"
      },
      "source": [
        "# Exercises \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWZRPwCw1xTo"
      },
      "source": [
        "**1. Data Cleaning**\n",
        "\n",
        "You will work with the same dataset as in Assigment 5.\n",
        "The dataset has address \n",
        "\n",
        "`url='https://github.com/amoreira2/Lectures/blob/main/assets/data/Assignment5.xlsx?raw=true'`\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "- Import pandas, numpy, matplotlib, and load the data set.\n",
        "- Import the datasets of industry returns and risk free rate.\n",
        "- Parse the date.\n",
        "- Set the index.\n",
        "- Drop missing observations.\n",
        "- Construct a dataframe with only excess returns.\n",
        "- Call this dataframe with the 49 excess returns time series `df`.\n",
        "- Call `df.head()` to check if everything works\n",
        "\n",
        "**_Hint:_**\n",
        "You did it in assignment 5, simply copy and paste your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El3e4csR1xTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "\n",
        "# your code below\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ke8n1Nw1xTq"
      },
      "source": [
        "**2. Expected excess return estimation**\n",
        "\n",
        "Compute the sample mean as the estimators for the expected excess returns of the 49 assets. \n",
        "\n",
        "Call this `ERe`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3f9QGzA1xTq"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7wvamWs1xTr"
      },
      "source": [
        "**3. Expected excess return uncertainty**\n",
        "\n",
        "We will now construct an estimator for the amount of uncertainty in our sample mean estimator. If we assume that each individual asset is uncorrelated over time (not terrible assumption), then the variance of the mean is\n",
        "\n",
        "$$var(\\bar{r_i}) = var(\\frac{\\sum_t^T r_{i,t}}{T})=\\frac{\\sum_t^T var( r_{i,t})}{T^2} = \\frac{var(r_{i,t})}{T}$$\n",
        "\n",
        "So all you need is the sample size (T) and the variance of each asset to obtain the varaince of our estimator.\n",
        "\n",
        "Please use this formula to compute the **STANDARD DEVIATION** of sample average estimator of each 49 asset. Call this `ERe_se`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lKPx44_1xTs"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe_se.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJi0bAYO1xTs"
      },
      "source": [
        "**4. Constructing a confidence interval for the expected excess return, part 1**\n",
        "\n",
        "We will now want to construct the 95% confidence interval for our estimator. The interval is such that it contains the true mean 95% of the time.\n",
        "\n",
        "The way to do this is to use the normal distribution CDF to figure out the threshold that leaves only 2.5% probability at the each side of the tails.\n",
        "\n",
        "Why 2.5% and not 5%? Because it is symmetric, so there is 2.5% probability in the left tail and 2.5 % in the right tail so overall there is only 5% probability that the expected return is out of the interval. Thus there is 95% probability that it is in the interval.\n",
        "\n",
        "In this exercise, you will find the threshold by doing the followings:\n",
        "\n",
        "1. import the stats library from the scipy package with `from scipy import stats`\n",
        "\n",
        "2. get the standard normal distribution with `sn=stats.norm(0,1)`, where 0 is the mean and 1 is the standard deviation\n",
        "\n",
        "3. get the threshold by using inverse cumulative distribution function for the appropriate `prob_value` to create a 95% CI (see discussion above). `threshold=sn.isf(prob_value)`\n",
        "\n",
        "4. make sure that this threshold is positive (if you got from the left tail, you will have to take the absolute value or just get from the right tail).\n",
        "\n",
        "5. make sure you did things correctly by calling `print(threshold)`.\n",
        "\n",
        "**_Hint:_**\n",
        "\n",
        "You can always check in the normal table that you did things correctly https://en.wikipedia.org/wiki/Normal_distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u2_K1z51xTt"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "print(threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q0HgrRl1xTu"
      },
      "source": [
        "**5. Constructing a confidence interval for the expected excess return, part 2**\n",
        "\n",
        "Armed with these threshold you can construct the interval as follows\n",
        "\n",
        "$$[\\bar{r}-threshold\\times\\sigma(\\bar{r}),\\bar{r}+threshold\\times\\sigma(\\bar{r})]$$\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. create an empty dataframe which has the names of industries as index and 'lower' and 'upper' as column names. Name it `ERe_ci`.\n",
        "\n",
        "2. construct the lower bound of the interval, $\\bar{r}-threshold\\times\\sigma(\\bar{r})$, and store it in the column of 'lower'\n",
        "\n",
        "3. compute the upper bound symmetrically, and store it in the column of 'upper'\n",
        "\n",
        "4. call `ERe_ci.head()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq98y8gc1xTu"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe_ci.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIWJeB2S1xTv"
      },
      "source": [
        "**6. Compute the tangency portfolio weights for a portfolio with annualized volatility of 10%**\n",
        "\n",
        "Store these in a dataframe whose rows are the names of the assets and the first column has the label 'mve_data'.\n",
        "\n",
        "Name this data frame `Weights`.\n",
        "\n",
        "`print(Weights)`\n",
        "\n",
        "TIP: You did this in Assigment 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8R8gk7N1xTv"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "print(Weights.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOSnMt-p1xTv"
      },
      "source": [
        "**7. Sensitivity to uncertainty of the Tangent portfolio calculation**\n",
        "\n",
        "Now we will compute the tangency portfolio but using a slightly different estimate for the mean. \n",
        "\n",
        "Do the followings.\n",
        "\n",
        "1. instead of using the sample mean for each asset, first we will pick one asset, `Hlth`. \n",
        "\n",
        "2. change its mean to be its lower bound of CI in Exercise 5 and then recalculate the tangency portfolio weights.\n",
        "\n",
        "3. store this in dataframe Weights with the column name `mve_Hlth-1.95`.\n",
        "\n",
        "4. create another column with weights computed from the perturbation in which its mean is changed to be the upper bound of CI, label this column `mve_Hlth+1.95 `. \n",
        "\n",
        "5. do a bar plot of these three sets of weights using Weights.plot.bar().\n",
        "\n",
        "Discuss what you notice in the bar plot:\n",
        "\n",
        "1. How much do the weights change?\n",
        "\n",
        "2. Which assets are impacted? Why?\n",
        "\n",
        "**_Hint:_** \n",
        "\n",
        "you might want to create a copy of your ERe estimator before you do the perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rjCGeFy1xTw"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5XhSWJl1xTw"
      },
      "outputs": [],
      "source": [
        "# your discussion below\n",
        "\n",
        "# 1. How much do the weights change?\n",
        "\n",
        "# 2. Which assets are impacted? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HUt9Od1xTw"
      },
      "source": [
        "**8. Performance impact of estimation uncertainty**\n",
        "\n",
        "\n",
        "Your Weight dataframe has 3 different weight schemes.\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. compute the in-sample Sharpe Ratio for these 3 different weight schemes.\n",
        "\n",
        "2. discuss the results that you obtained\n",
        "\n",
        "**_Hint:_**\n",
        "You should use the **real** data (e.g. `df`,`ERe`,`CovRe` ) to compute the Sharpe ratio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSx2PCPz1xTx"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFhbKApV1xTx"
      },
      "outputs": [],
      "source": [
        "# your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GksOAMW1xTx"
      },
      "source": [
        "**9. Reporduce analysis of Exercises 7-8 for all assets**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. use a for loop to loop through the 49 portfolios and create the \"perturbed\" weights and the Sharpe Ratio of the perturbed weights. \n",
        "\n",
        "2. record for each asset the average drop in the Sharpe Ratio associated with the perturbation in the tangency portfolio weights. \n",
        "\n",
        "3. store the results in a dataframe named `dSR` (difference in SR):\n",
        "$$dSR[asset]=\\frac{1}{2}\\frac{SR(asset+1.95)+SR(asset-1.95)}{SR(data)}$$\n",
        "where SR(asset+1.95) and SR(asset-1.95) were the Sharpe ratios obtained when you perturb the expected excess return of that asset to the upper and lower bound of the CI.\n",
        "`dSR` should be a dataframe with industry names as index and a column, called `SR_change`, containing the results of the calculation from the expression above.\n",
        "\n",
        "4. do a bar plot of this Sharpe ratio change.\n",
        "\n",
        "Discuss the bar plot:\n",
        "\n",
        "1. What do you think is the key takeaway from the analysis above\n",
        "\n",
        "***Hint:***\n",
        "Note that all you need to do here is to get the code you developed above and adapt it to work with a for loop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHiytEDX1xTy"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY2TuV-x1xTy"
      },
      "outputs": [],
      "source": [
        "# your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp9iH3Nm1xTy"
      },
      "source": [
        "**10. Monte Carlo, part 1**\n",
        "\n",
        "So far our focus is on the estimation uncertainty of risk premiums. Covariance matrices also need to be estimated.\n",
        "\n",
        "You will now implement a Monte-Carlo method to evaluate the overall uncertainty in the construction of the tangency portfolio.\n",
        "\n",
        "You already have the sample estimates from the vector of expected excess returns `ERe` and the variance-covariance matrix `CovRe`  (you used those in Exercise 6).\n",
        "\n",
        "Now you use the function `np.random.multivariate_normal` to simulate draws from a multivariate normal distribution with vector of mean equal to `ERe` and the covariance matrix equal to `CovRe`.\n",
        "\n",
        "Do the following:\n",
        "\n",
        "1. write the code that draws ONE realization of returns for this set of 49 assets. \n",
        "\n",
        "***Hint:***\n",
        "\n",
        "you should get a vector 49 by 1 that changes every time you run the cell.\n",
        "\n",
        "type `np.random.multivariate_normal?` to see how this function works\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku2xyycV1xTy"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD33YJYC1xTy"
      },
      "source": [
        "**11. Monte Carlo, part 2**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. now set the parameter `size` in the `multivaraite_normal` function to draw T realizations of the 49 assets, where you set T to the number of months you have in the data set.\n",
        "\n",
        "2. print the shape of your draw. This should return you a T by N matrix of returns, something with exactly the same shape as our data set. \n",
        "\n",
        "***Hint:***\n",
        "every time you run the cell again you get a different realization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BIZ7lNp1xTy"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8E6w9nL1xTz"
      },
      "source": [
        "**12. Monte Carlo, part 3**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. copy the code above, so you have a simulated sample of monthly industry returns.\n",
        "\n",
        "2. use the simulated return data and the weights in `mve_data` column of the dataframe `Weights` to construct a time-series of portfolio excess return. \n",
        "\n",
        "3. compute and print its Sharpe Ratio. \n",
        "\n",
        "***Hint:***\n",
        "\n",
        "Every time you run this cell you should get a different Sharpe Ratio. This variation reflects the amount of overall uncertainty built in our investment strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Z3CiIX1xTz"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgVHy9TC1xTz"
      },
      "source": [
        "**13. Monte Carlo, part 4**\n",
        "\n",
        "Now copy the code of the question above and write a foor loop around it. \n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. loop throught this code 1000 times and each time record the resulting Sharpe Ratio.\n",
        "\n",
        "2. cave this in a dataframe called `MC`.\n",
        "\n",
        "3. create a histogram of these Sharpe Ratios with 50 bins using the method `.hist`\n",
        "\n",
        "Discuss the plot:\n",
        "\n",
        "1. what do you conclude?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey7Znsls1xTz"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vMNnh711xTz"
      },
      "outputs": [],
      "source": [
        "# your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksduP9Z_1xT0"
      },
      "source": [
        "**14. Bootstrap**\n",
        "\n",
        "The Monte-Carlo approach assumes that the distribution of returns is normal which is a good approximation but not literally true. It turns out that we can use another approach called Bootstrap that instead of sampling from the normal distribution, we sample from the actual data. \n",
        "\n",
        "Basically, bootstrap approach randomly draws one observation (one month of 49 industy returns in our case) from the real dataset, treats it as a new observation in the simulated sample and repeats this process until the simulated sample has needed sample size. Observations are drawn with replacement, which means we draw from the whole real dataset every time so it is very likely that some data points are drawn more than once.  \n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. start with your code from question 13, \n",
        "\n",
        "2. instead of calling `np.random.multivariate_normal` to draw a sample, we will use the method `sample`\n",
        "\n",
        "    - refer to https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n",
        "    - set the parameter 'frac' to 1 so it draws a distribution of excactly same size as our original sample \n",
        "    - set the parameter 'replace' to 'True' so it samples with replacement (otherwise you will get exactly the same realizations, only in different order).\n",
        "\n",
        "3. now you can simply plug this realization in your code from question 13.\n",
        "\n",
        "4. save the results in a dataframe called `Boot`.\n",
        "\n",
        "5. create a histogram of these Sharpe Ratios with 50 bins using the method `.hist`\n",
        "\n",
        "Compare the results in 13 and 14, and explain:\n",
        "\n",
        "1. does the key takeway change?\n",
        "\n",
        "2. does the distribution of the SR of our strategy change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVFnD1ih1xT0"
      },
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF_gSdDC1xT0"
      },
      "outputs": [],
      "source": [
        "# your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExscHOuF1xT0"
      },
      "source": [
        "**15. Please explain why an investor should care about these results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvL1hqr91xT1"
      },
      "outputs": [],
      "source": [
        "# your answer below\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "toc-autonumbering": true
  },
  "nbformat": 4,
  "nbformat_minor": 0
}